{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first install python 3.6\n",
    "!sudo apt-get update -y\n",
    "!sudo apt-get install python3.6\n",
    "# change alternatives\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 1\n",
    "# select python version\n",
    "!sudo update-alternatives --config python3\n",
    "# check python version\n",
    "!python --version\n",
    "# install pip for new python \n",
    "!sudo apt-get install python3.6-distutils\n",
    "!wget https://bootstrap.pypa.io/get-pip.py\n",
    "!python get-pip.py\n",
    "# upgrade pip\n",
    "!sudo apt install python3-pip\n",
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sentence=list()\n",
    "Y_sentence=list()\n",
    "count=0\n",
    "base_path=\"/home/lorenzobgl/projects/NER_Manufacturing-FabNER\"\n",
    "X_train=list()\n",
    "Y_train=list()\n",
    "X_val=list()\n",
    "Y_val=list()\n",
    "X_test=list()\n",
    "Y_test=list()\n",
    "with open(base_path+\"/S1-test.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        if(line!='\\n'):\n",
    "            X_sentence.append(line.split(\" \")[0])\n",
    "            Y_sentence.append(line.split(\" \")[1][:-1])\n",
    "        else:\n",
    "            count+=1\n",
    "            if(count==2):\n",
    "                X_test.append(X_sentence)      \n",
    "                Y_test.append(Y_sentence)        \n",
    "                X_sentence=list()\n",
    "                Y_sentence=list()\n",
    "                count=0\n",
    "with open(base_path+\"/S2-train.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        if(line!='\\n'):\n",
    "            X_sentence.append(line.split(\" \")[0])\n",
    "            Y_sentence.append(line.split(\" \")[1][:-1])\n",
    "        else:\n",
    "            count+=1\n",
    "            if(count==2):\n",
    "                X_train.append(X_sentence)      \n",
    "                Y_train.append(Y_sentence)        \n",
    "                X_sentence=list()\n",
    "                Y_sentence=list()\n",
    "                count=0\n",
    "with open(base_path+\"/S3-val.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        if(line!='\\n'):\n",
    "            X_sentence.append(line.split(\" \")[0])\n",
    "            Y_sentence.append(line.split(\" \")[1][:-1])\n",
    "        else:\n",
    "            count+=1\n",
    "            if(count==2):\n",
    "                X_val.append(X_sentence)      \n",
    "                Y_val.append(Y_sentence)        \n",
    "                X_sentence=list()\n",
    "                Y_sentence=list()\n",
    "                count=0\n",
    "# sentences it is a list of sentences. Every sentence is a list of tuple\n",
    "tokenized_sentences=X_train+X_test+X_val\n",
    "tokenized_labels=Y_train+Y_test+Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences_modified=[]\n",
    "technical_word=\"\"\n",
    "for sent in tokenized_sentences:\n",
    "    tokenized_sent_modified=[]\n",
    "    for el in sent:\n",
    "        if el[1][0]==\"O\" or el[1][0]==\"S\":\n",
    "            tokenized_sent_modified.append(el[0])\n",
    "        else:\n",
    "            if el[1][0]==\"B\" or el[1][0]==\"I\":\n",
    "                technical_word=technical_word+el[0]+\"_\"\n",
    "            if el[1][0]==\"E\":\n",
    "                tokenized_sent_modified.append(technical_word+el[0])\n",
    "                technical_word=\"\"\n",
    "    tokenized_sentences_modified.append(tokenized_sent_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# model_1=FastText(sentences=tokenized_sentences_modified, vector_size=300, window=10, min_count=3, sg=1, negative=10, alpha=0.01, sample=0.0001)\n",
    "model_2=FastText(sentences=tokenized_sentences, vector_size=100, window=10, min_count=5, sg=1, negative=10, alpha=0.01, sample=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3047633, 6991040)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_1.train(tokenized_sentences_modified, total_examples=len(tokenized_sentences_modified), epochs=20)\n",
    "model_2.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_3=Word2Vec(sentences=tokenized_sentences_modified, vector_size=300, window=10, min_count=5, sg=1, negative=10, alpha=0.01, sample=0.0001)\n",
    "# model_3.train(tokenized_sentences_modified, total_examples=len(tokenized_sentences_modified), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from modelli import wrapper, utils\n",
    "importlib.reload(wrapper)\n",
    "from modelli.wrapper import Sequence\n",
    "\n",
    "\n",
    "lstm=Sequence(embeddings=model_2.wv, initial_vocab=model_2.wv.key_to_index)\n",
    "lstm.fit_vocab(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.p._word_vocab.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelli.wrapper import Sequence\n",
    "import pickle\n",
    "\n",
    "lstm=Sequence.load(\"param\",\"weights.h5\",\"preprocessor\")\n",
    "lstm.model.compile( optimizer=lstm.optimizer)\n",
    "with open('optimizer.pkl', 'rb') as f:\n",
    "    weight_values = pickle.load(f)\n",
    "lstm.model.optimizer.set_weights(weight_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "131/295 [============>.................] - ETA: 2:14 - loss: 8.1768"
     ]
    }
   ],
   "source": [
    "from modelli.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(lstm.model, preprocessor=lstm.p)\n",
    "trainer.train(X_train, Y_train, X_val, Y_val,epochs=1, batch_size=32,verbose=1,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import batch_get_value\n",
    "import pickle\n",
    "\n",
    "# lstm.save(\"param\",\"weights.h5\",\"preprocessor\")\n",
    "symbolic_weights = getattr(lstm.model.optimizer, 'weights')\n",
    "weight_values = batch_get_value(symbolic_weights)\n",
    "with open('optimizer.pkl', 'wb') as f:\n",
    "    pickle.dump(weight_values, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelli.wrapper import Sequence\n",
    "\n",
    "lstm=Sequence (word_embedding_dim=100, embeddings=model_2.wv)\n",
    "lstm.fit(X_train, Y_train, X_val, Y_val, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelli.models import save_model\n",
    "\n",
    "save_model(lstm.model, \"weights.h5\",\"param\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.fit(X_train, Y_train, X_val, Y_val,epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anago",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e31fa40861c59aaf1b6289d1c109d7e37c48dfedeec87a9d5e4e36cd093f8931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
